{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Task 1: one hidden layer NN</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "x_train = x_train.reshape(x_train.shape[0],-1).T\n",
    "x_test = x_test.reshape(x_train.shape[0],-1).T\n",
    "x_train =x_train.T\n",
    "x_test = x_test.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_training = []\n",
    "import numpy as np\n",
    "array=np.zeros((y_train.shape[0],10))\n",
    "for i in range(y_train.shape[0]):\n",
    "    j=int(y_train[i])\n",
    "    array[i,j]=1\n",
    "    y_trains=array\n",
    "    y_training=y_trains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll=0.03\n",
    "m=len(x_train)\n",
    "w1=np.random.randn(784,24)\n",
    "b1=np.random.randn(1,24)\n",
    "w2=np.random.randn(24,10)\n",
    "b2=np.random.randn(1,10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "tracking=[]\n",
    "\n",
    "for i in range(1000):\n",
    "    z1=np.dot(x_train, w1)+b1  \n",
    "    a1=np.tanh(z1)  \n",
    "    z2= np.dot(a1,w2) +b2 \n",
    "    a2=softmax(z2)\n",
    "    loss=-np.sum(np.multiply(y_train1,np.log(a2+0.01)))/m \n",
    "    tracking.append(loss)\n",
    "    dz2=a2-y_train1  \n",
    "    dw2=((np.matmul(a1.T,dz2))/m)\n",
    "    db2=(1/m)*(np.sum(dz2,axis=1,keepdims=True))\n",
    "    da1=np.matmul(dz2,w2.T)\n",
    "    dz1=sigmoid(z1)*(1-(sigmoid(z1)))*(da1)\n",
    "    dw1=np.matmul(x_train.T,dz1)/m\n",
    "    db1=(1/m)*(np.sum(dz1,axis=1,keepdims=True))\n",
    "    w2=w2-(ll*dw2)\n",
    "    b2=b2- ll*db2    \n",
    "    w1=w1-ll*dw1   \n",
    "    b1=b1-ll*db1  \n",
    "    if (i%10==0):\n",
    "      print(loss, metrics.accuracy_score(np.argmax(y_trainglobal,axis=1),np.argmax(a2.T,axis=0))*100)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zt1=np.dot(x_train.T,w1)+b\n",
    "at1 = sigmoid(zt1)\n",
    "z2= np.dot(at1,w2)+b\n",
    "at2=softmax(z2)\n",
    "test_prediction = np.argmax(at2,axis=1).T\n",
    "test_labels=np.argmax(y_training,axis=1).T\n",
    "cf=confusion_matrix(test_prediction,test_labels)\n",
    "print(cf)\n",
    "count=0\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        if i==j:\n",
    "            count+=cf[i][j]\n",
    "accuracy = count/len(x_train[0])*100\n",
    "print(accuracy)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(tracking)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.title(\"Epoch Vs Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Task 2: Multi hidden layer using Keras</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=tf.keras.utils.normalize(x_train,axis=1)\n",
    "x_test=tf.keras.utils.normalize(x_test,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2=tf.keras.models.Sequential()\n",
    "model2.add(tf.keras.layers.Flatten())\n",
    "model2.add(tf.keras.layers.Dense(64,activation=tf.nn.sigmoid))\n",
    "model2.add(tf.keras.layers.Dense(32,activation=tf.nn.sigmoid))\n",
    "model2.add(tf.keras.layers.Dense(10,activation=tf.nn.softmax))\n",
    "model2.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics = ['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h=model2.fit(x_train,y_train,epochs=8,validation_data = (x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(h.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(h.history['acc'])\n",
    "plt.plot(h.history['val_acc'])\n",
    "plt.title('Epoch Vs Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(h.history['loss'])\n",
    "plt.plot(h.history['val_loss'])\n",
    "plt.title('Epoch vs Loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model2.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "pred=[]\n",
    "for i in range(len(x_test)):\n",
    "    pred.append(np.argmax(predictions[i]))\n",
    "confusion_matrix_final=confusion_matrix(y_test,pred)\n",
    "print(confusion_matrix_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>Task 3: Implementing CNN</H3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "x_train = x_train.reshape(x_train.shape[0], 28,28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28,28, 1)\n",
    "\n",
    "print(x_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = tf.keras.Sequential()\n",
    "model3.add(tf.keras.layers.Conv2D(filters=64, kernel_size=2, activation='relu', input_shape=(28,28,1))) \n",
    "model3.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
    "model3.add(tf.keras.layers.Dropout(0.3))\n",
    "model3.add(tf.keras.layers.Conv2D(filters=32, kernel_size=2,activation='relu'))\n",
    "model3.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
    "model3.add(tf.keras.layers.Dropout(0.3))\n",
    "model3.add(tf.keras.layers.Flatten())\n",
    "model3.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "model3.add(tf.keras.layers.Dropout(0.5))\n",
    "model3.add(tf.keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.compile(loss='sparse_categorical_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h=model3.fit(x_train,\n",
    "         y_train,\n",
    "         batch_size=64,\n",
    "         epochs=10,validation_data = (x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(h.history['acc'])\n",
    "plt.plot(h.history['val_acc'])\n",
    "plt.title('Epoch Vs Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(h.history['loss'])\n",
    "plt.plot(h.history['val_loss'])\n",
    "plt.title('Epoch vs Loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "predictions=model3.predict(x_test)\n",
    "pred=[]\n",
    "for i in range(len(x_test)):\n",
    "    pred.append(np.argmax(predictions[i]))\n",
    "confusion_matrix_final=confusion_matrix(y_test,pred)\n",
    "print(confusion_matrix_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
